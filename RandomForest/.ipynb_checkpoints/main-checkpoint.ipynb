{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b15592-921a-407e-ac99-d62ec6a9622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris,load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c07043-a56e-4b3c-bac5-215d1f22d80a",
   "metadata": {},
   "source": [
    "### Intuition \n",
    "***\n",
    "a random forest uses baging and combines many desicion trees to one big estimate. Each individual tree is constructed randomly: on random sample of input points and by selecting the spliting point from a random subset of all dimensions.\n",
    "</br>\n",
    "\n",
    "### Algorithm\n",
    "***\n",
    "for $b=1$ to $B$ </br>\n",
    "- Draw a bootstrap sample Z of size N from the training data\n",
    "- Gro a random forest tree $T_{b}$ to the bootstrapped data, by recursively repeating the following steps for each terminal node of the tree, until the minimum node size $n_{min}$ is reached \n",
    "  - Select m variables randomly from the p variables\n",
    "  - Select the best variable/split-point among the m\n",
    "  - Split the node into two daughter nodes\n",
    "  \n",
    "Output the ensambles of tree model $\\{T_{b}\\}_{1}^{B}$ </br>\n",
    "To make a prediction at the new point $x$: </br>\n",
    "$\\quad$ Regression: $f(x) = \\frac{\\sum_{1}^{B} T_{b}}{B}$</br>\n",
    "$\\quad$ classification: $f(x) = argmax(T_{b})$\n",
    "\n",
    "### Parameters\n",
    "***\n",
    "The size of the subsample: reasonably large; can be with or without replacement (in the latter case, one often chooses the subsample size equal to the size of the original sample). The number m of dimensions of which we pick the be one: Typically, people choose something around $d/3$ where ber d is the original dim of the data. The number B of trees should be large. The number $n_{min}$ of points in the leaves: depends on whether you consider deep or shallow trees. In the extreme case of deep trees, $n_{min} = 1$ Then you need many trees B. In the case of shallow trees, $n \\approx log(n)$\n",
    "\n",
    "\n",
    "### Consistency of random ferest\n",
    "***\n",
    "On a high level, here are the main results:\n",
    "- A single spatial decision tree is consistent if the diameter of all cell converges to 0 and at the same time, the number of points in each of the cells tends to infinity, as the number n of the data points goes to infinity \n",
    "- If all individual trees are consistent, so is the random forest \n",
    "- Curiously, a random forest can be consistent even if all its trees are not consistent. This is particularly the case for deep trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
