{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ce605615-ae9a-45e7-aba3-de6ca71ccfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris,load_diabetes, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import numpy as np \n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4a7ca-591b-40c6-b4f9-1474bf3a328f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Intuition \n",
    "***\n",
    "Intuitively a strong learner is a classification algorithm that can approximate the true solution up to a small error $\\epsilon$. The goal of machine learning is to construct strong learners. However, they are often hard to construct and computationally expensive. A \"Weak learner\" is an algorithm that is just slightly better than random guessing: for a classification problem with balanced classes, its 0-1-loss is just a tiny bit better than random guessing: $50\\% + \\epsilon$ The idea of boosting is to combine many weak learners to obtain a strong learner \n",
    "\n",
    "\n",
    "### The outline\n",
    "***\n",
    "- Given a training set of $n$ points $(x_i,y_i)$ the boosting algorithm proceeds in the $t$ rounds.\n",
    "- Training points have weights that change from round to round. The weights always add up to one.\n",
    "- In each round, we train the weak classifier on the training points with the current weights. We then update the weights: \n",
    "    - For points $x_i$ that were misclassified, we increase their weight $w_i$ \n",
    "    - For points $x_i$ that got correctly classified we decrease their weights $w_i$\n",
    "- At the very end the final classifier is a weighted sum, some kind of weighted majority vote, of the weak classifiers of each round.\n",
    "\n",
    "\n",
    "### Some remarks \n",
    "***\n",
    "- As opposed to random forest, which generates randomness through subsampling points and dimensions, the training set in boosting is always the same. We just change its weights.\n",
    "- By re-weighting, the algorithm can focus on those examples that it finds difficult, or on the aspect that has been overlooked so far. \n",
    "- By the definition of a weak classifier, once a data point has accumulated weights larger than $50\\%$ the weak algorithm will get it right. But it is not obvious that this helps the classifier because there are many points that we need right... So the question is how to combine all the evidence of the weak classifiers in such a way that we obtain a strong classifier at the end \n",
    "\n",
    "\n",
    "### AdaBoost algorithm\n",
    "***\n",
    "<strong>Input:</strong></br>\n",
    "$\\quad$ Training set: $\\{S|s_{i} = (x_i, y_i) \\forall i\\}$ </br>\n",
    "$\\quad$ Weak Learner: $WL$ </br>\n",
    "$\\quad$ Number of rounds: $T$ </br>\n",
    "\n",
    "<strong>Initialize:</strong></br>\n",
    "$\\quad$ $D^{(1)} = (\\frac{1}{m},...,\\frac{1}{m})$\n",
    "\n",
    "<strong>for $t=1,...T$:</strong></br>\n",
    "$\\quad$ Invoke weak learner $h_{t} = WL(D^{(t)},S)$ </br>\n",
    "$\\quad$ Compute $\\epsilon_{t} = \\sum_{i=1}^{m}D^{(t)} \\mathbb{1}_{[y_{i} \\neq h_{t}(x_i)]}$ </br>\n",
    "$\\quad$ let $w_{t} = \\frac{1}{2}log(\\frac{1}{\\epsilon_{t}} - 1)$ </br>\n",
    "$\\quad$ Update $D_{i}^{(1+t)} = \\frac{ D_{i}^{(t)} \\exp\\left(-w_{t}y_{i}h_{t}(x_i)\\right)}{\\sum_{j=1}^{m}\\left(D_{j}^{(t)}\\exp(-w_{t}y_{j}h_{t}(x_j))\\right)}$ for all $i=1,...,m$</br>\n",
    "\n",
    "<strong>Output:</strong></br>\n",
    "$\\quad$ The hypothesis $h_{s}(x) = sign \\left( \\sum_{t=1}^{T}w_{t} h_{t}(x)\\right)$\n",
    "\n",
    "\n",
    "### Theoretical resoures\n",
    "***\n",
    "$(*)$ https://www.youtube.com/watch?v=4B5sp5Hw9Qc&list=PL05umP7R6ij2XCvrRzLokX6EoHWaGA2cC&index=28 </br>\n",
    "$(*)$ https://en.wikipedia.org/wiki/AdaBoost </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "8bf045ca-1909-4031-8d63-8aad73eb768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, _, desc, feature_names, _, _ = load_breast_cancer().values()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "b216da72-f74f-4aaf-ac24-9343267ae2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None\n",
    "        \n",
    "                      \n",
    "        \n",
    "class DesicionTree:\n",
    "    \n",
    "    def __init__(self, max_depth=100, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.weights = None\n",
    "        self.root = None\n",
    "        self.criteria = dict(\n",
    "            gini = lambda Y,W : np.sum([np.log2(v/len(Y)) * v/len(Y) \\\n",
    "                                        for v,w in zip(Counter(Y).values(), W) ] / np.sum(W)),\n",
    "            l2   = lambda x: x,\n",
    "        )\n",
    "    \n",
    "    def fit(self, X, y, sample_size=1, weights=None):\n",
    "        self.weights = np.array([1/len(y) for _ in range(len(y))]) if(weights is None) else weights \n",
    "        self.root = self._build_tree(X, y, sample_size)\n",
    "        return self\n",
    "        \n",
    "    def _information_gain(self, n, total_loss, n_left, left_loss, n_rigth, right_loss):\n",
    "        # Calculate information gain\n",
    "        if n_left == 0 or n_rigth == 0: \n",
    "            return 0\n",
    "        return total_loss - ((n_left / n) * left_loss + (n_rigth / n) * right_loss)\n",
    "    \n",
    "    def _create_split(self, data, split):\n",
    "        # Split data based on split point\n",
    "        left_idx = np.argwhere(data <= split).flatten()\n",
    "        right_idx = np.argwhere(data > split).flatten()\n",
    "        return left_idx, right_idx\n",
    "        \n",
    "    \n",
    "    def _get_split(self, X, y, sample_size, criteria='gini'):\n",
    "        score = []      \n",
    "        total_loss = self.criteria[criteria](y, self.weights)\n",
    "        features = np.random.choice(X.shape[1], size= math.ceil(X.shape[1] * sample_size), replace=False)\n",
    "        for dimension, data in zip(features, X[:, features].T):\n",
    "            for split in np.unique(data):\n",
    "                # Get indices of each group given the split\n",
    "                left_idx, right_idx = self._create_split(data, split)\n",
    "\n",
    "                # Get loss of each group given the split\n",
    "                left_loss = self.criteria[criteria](y[left_idx], self.weights)\n",
    "                right_loss = self.criteria[criteria](y[right_idx], self.weights)\n",
    "                \n",
    "                # Calculate information gain\n",
    "                ig = self._information_gain(len(y) ,total_loss, len(left_idx), left_loss, len(right_idx), right_loss)\n",
    "                score.append((ig, dimension, split))   \n",
    "                \n",
    "        return min(score, key = lambda e: e[0])\n",
    "    \n",
    "    \n",
    "    def _build_tree(self, X, y, sample_size, depth=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.n_class_labels = len(np.unique(y))\n",
    "        \n",
    "        (ig, dimension, split) = self._get_split(X, y, sample_size)\n",
    "        left_idx, right_idx = self._create_split(X[:, dimension], split)\n",
    "        \n",
    "        # stopping criteria\n",
    "        if (depth >= self.max_depth \n",
    "            or self.n_class_labels == 1 \n",
    "            or self.n_samples < self.min_samples_split\n",
    "            or len(left_idx) == 0 \n",
    "            or len(right_idx) == 0      \n",
    "           ):\n",
    "            most_common_Label = Counter(y).most_common(1)[0][0]  \n",
    "            return Node(value=most_common_Label)\n",
    "        else:\n",
    "            left_child = self._build_tree(X[left_idx, :], y[left_idx], sample_size, depth + 1)\n",
    "            right_child = self._build_tree(X[right_idx, :], y[right_idx], sample_size, depth + 1)\n",
    "            return Node(feature=dimension, threshold=split, left=left_child, right=right_child)\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf(): \n",
    "            return node.value  \n",
    "        if x[node.feature] <= node.threshold: \n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)\n",
    "    \n",
    "    \n",
    "class AdaBoost:\n",
    "    \n",
    "    def __init__(self, max_depth=100, min_samples_split=2, size=10):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.weights = None\n",
    "        self.epsilon = None\n",
    "        self.size = size\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        m = len(y)\n",
    "        y = np.array([1 if e == 1 else -1 for e in y])\n",
    "        \n",
    "        self.trees = [DesicionTree(max_depth = 100) for _ in range(self.size)] \n",
    "        self.epsilon = np.zeros(self.size)\n",
    "        \n",
    "        w = np.array([ (1/m) for _ in range(m)])\n",
    "        for t, tree in enumerate(self.trees):\n",
    "            h = tree.fit(X,y,weights=w).predict(X)\n",
    "            e = np.sum([weight for y_pred, y_truth, weight in zip(h,y,w) if y_pred == y_truth])         \n",
    "            epsilon = (1/2) * np.log((1/e)-1) \n",
    "            self.epsilon[t] = epsilon\n",
    "            w = (w * np.exp( -y * epsilon * h)) \n",
    "            w = w / np.sum(w)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for tree, epsilon in zip(self.trees, self.epsilon):\n",
    "            predictions.append(tree.predict(X) * epsilon)\n",
    "        f = np.sum( np.array(predictions), axis=0 )\n",
    "        return(np.sign(f))\n",
    "            \n",
    "            \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "033892ad-e50f-4997-a8b5-c51f48ed3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AdaBoost(size=2);\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
